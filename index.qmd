---
title: "Métodos computacionales para las ciencias sociales"
subtitle: "Procesamiento de texto IV"
format: 
    revealjs:
      auto-stretch: false
      scrollable: true
      link-external-newwindow: true
css: style.css
editor: source
execute:
  echo: false
  python:
    executable: ./.venv/bin/python
---

```{r, echo=FALSE}
library(kableExtra)
library(tidyverse)
```

## Temas

Tópicos sobre análisis de texto

-   Diversidad léxica
-   *Keyness*
-   *Wordfish*

## Diversidad léxica

**Propuesta de investigación**

Nos interesa conocer la producción de textos de alumnos de tercero básico

. . .

Un indicador posible (entre otros) es la riqueza léxica

. . .

**Supuesto**: la riqueza léxica está asociada las habilidades de expresión escrita

. . .

**Podemos estudiar cuántas palabras diferentes se usan en un texto**

## Ejemplo de juguete

```{r, echo=TRUE}
library(quanteda.textstats)
library(quanteda)
texto1 <-  c("juego juego juego juego")
texto2 <- c("canto canto canto canto")
texto3 <- c("juego canto juego canto")

ejemplo <- data.frame(text = c(texto1, texto2, texto3))

ejemplo %>% 
  corpus() %>% 
  tokens() %>% 
  dfm() %>% 
  textstat_lexdiv()



```

## Aplicación real

::: panel-tabset
## Texto

```{r, echo=FALSE}
library("readtext")
allende <- readtext("data/discurso_allende.txt")  
```

```{r, echo=FALSE}
discurso <- allende$text %>% 
  str_split(pattern = " ")
paste(discurso[[1]][1:13], collapse = " ")

```

## riqueza 1

```{r, echo=TRUE}
library("readtext")
allende <- readtext("data/discurso_allende.txt")  

dfm_mat <- allende %>% 
  corpus() %>% 
  tokens() %>% 
  dfm() 
  
diversidad <- textstat_lexdiv(dfm_mat)
print(diversidad)

```

## riqueza 2

```{r, echo=TRUE}
dfm_mat <- allende %>% 
  corpus() %>% 
  tokens() %>% 
  tokens_select(stopwords("es"), selection = "remove" ) %>% 
  dfm() 
  
diversidad <- textstat_lexdiv(dfm_mat)
print(diversidad)

```

**¿Y si lematizamos?**

## ideas

Comparación con otros políticos de la época

Comparación en el tiempo

Comparación entre tendencias políticas

¿Más ideas?
:::

## Análisis de frecuencia relativa (keyness)

Estamos interesados en comparar textos

. . .

Exploraremos noticias en inglés del Guardian (política, sociedad e internacional. 2012-2016)

```{r, echo=TRUE}
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(lubridate)
require(quanteda.corpora) ## solo disponible en github
```

## Descargar y procesar

```{r, echo=TRUE}
corpus_guardian <- download("data_corpus_guardian")
tokens <- tokens(corpus_guardian, remove_punct = TRUE) 
dfm <- dfm(tokens)
print(dim(dfm)) 
```

6000 noticia procesadas

. . .

```{r, echo=TRUE}
tstat_key <- textstat_keyness(dfm, 
                              target = year(dfm$date) >= 2016)
textplot_keyness(tstat_key)

```

## Wordfish (Slapin & Proksch, 2008)

Modelo no supervisado para hacer posicionamiento ideológico

. . .

Nos permite posicionar textos en un espacio de una dimensión

. . .

Se basa únicamente en la frecuencia de las palabras

. . .

$y_{ijt}=Poisson(\lambda_{ijt})$

$\lambda_{ijt} = exp(\alpha_{it} + ψ_{j} + β_j ∗ ω_{it})$

$j$: palabra

$i$: partido

$\alpha_{i}$: efecto fijo partido (textos muy largos)

$ψ_j$: efecto fijo palabra (palabras usadas mucho por todos los partidos)

$ω$: Parámetro que indica la posición de un actor/partido

$β$: Poder discriminador de las palabras

## Implementación wordfish

::: panel-tabset
## Datos

Declaraciones de principios de partidos

Obligación de publicarse por Ley de Transparencia

Tenemos algunos en pdf y otros en txt

## Carga

```{r, echo=TRUE}
library(readtext)
library(tidyverse)

# Cargar datos
data <- readtext("data/partidos/*.pdf") # algunos programas están en pdf
data2 <- readtext("data/partidos/*.txt") # otros están en txt 

data <- data %>% 
  bind_rows(data2)

```

## edición

```{r, echo=TRUE}
# Sacar caracteres molestos
data_edit <- data %>% 
  mutate(text = str_replace_all(text, pattern = "\\n", " "),
         text = tolower(text)
         ) %>% 
  mutate(doc_id = case_when(
    doc_id == "Declaración_Principios_PC.pdf" ~ "pc",
    doc_id == "Declracion-de-Principios-PSCH_2018.pdf" ~ "ps",
    doc_id == "declaracion_rd.pdf" ~ "rd",
    doc_id == "cs.txt" ~ "cs",
    doc_id == "DECLARACION-PRINCIPIOS-PDC.pdf"  ~ "dc",
    doc_id == "DECLARACIÓN DE PRINCIPIOS RENOVACIÓN NACIONAL.pdf" ~ "rn",
    doc_id == "evopoli.txt" ~ "evopoli",
    doc_id == "Declaracion de principios 2017 udi.pdf" ~ "udi",
    doc_id == "republicanos.txt" ~ "republicanos",
    doc_id == "radical.txt" ~ "radical"
  )) %>% 
  mutate(izquierda = if_else(doc_id %in% c("rd", "pc", "ps", "cs"), 1, 0 )) %>% 
  filter(izquierda == 1 | doc_id == "dc" | doc_id == "rn")

```

## quanteda

```{r, echo=TRUE}

tokens <- corpus(data_edit) %>% 
  tokens(remove_punct = TRUE, remove_numbers = T) %>%
  tokens_select(pattern = stopwords("es"), selection = "remove", min_nchar=3L) 

dfm <- tokens %>% 
  dfm()


```

## wordfish

```{r, echo=TRUE}
library(quanteda.textmodels)
wf <- textmodel_wordfish( dfm, dir = c(2, 1) ) # 

# Función de quanteda para crear gráfico
textplot_scale1d(wf)  


```

## gráfico propio

El parámetro $\theta$ corresponde a $ω$ en el modelo inicial

```{r, echo=TRUE}

# Construir intervalos de confianza 
theta <- data.frame(docs = wf$docs, theta = wf$theta, se = wf$se.theta) %>% 
  mutate(lower = theta -  1.96 * se,
         upper = theta +  1.96 * se
         )

theta %>% 
  mutate(docs = toupper(docs)) %>% 
  ggplot(aes(x =  reorder(docs, theta), y = theta)) +
  geom_point() +
  coord_flip() +
  geom_segment(aes(x = docs, y = lower , 
                   xend = docs, yend = upper ,
                   colour = "segment"))  +
  labs(title = "Puntajes wordfish a partir de declaración de principios de los partidos") +
  theme_bw() +
  theme(axis.text = element_text(size = 13),
        axis.title.y = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 18),
        panel.border = element_blank()
        ) 

```
:::

## Poder descriminador de palabras

```{r, fig.width=7, fig.height=5, fig.align='center',  echo=TRUE}

plot_words <-  textplot_scale1d(wf, margin="features", highlighted = c("comunista", "socialista", "recabarren", "revolucionarios", "empleo", "delictual", "religiosa", "chile"))
       
plot_words

```

## Topic modeling

LDA: Latent Dirichlet allocation

. . .

Cada documento es tratado como una mezcla de tópicos

. . .

Cada tópico es una mezcla de palabras

. . .

```{r, echo=TRUE}
library(topicmodels)

data("AssociatedPress")
AssociatedPress

```

. . .

```{r, echo=TRUE}
lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
lda
```

## Topic modeling

beta: Probabilidad de que una palabra sea generada por un tópico

```{r, echo=TRUE}
library(tidytext)
topics <- tidy(lda, matrix = "beta")
topics
```

## Topic modeling

```{r, echo=TRUE}
top_terms <- topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

## Topic modeling

```{r, echo=TRUE}
beta_wide <- topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_wide %>%
  mutate(new_log = abs(log_ratio)) %>% 
  slice_max(new_log, n = 10) %>% 
  ggplot(aes(x = fct_reorder(term, desc(new_log)) , y = log_ratio)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme(axis.title.y = element_blank())

```

## Bertopic

**200.000 noticias de medios chilenos**

<iframe src="topicos_noticias_prod.html" class="full-width-iframe" height="500" width="100%" style="border:none;">

</iframe>

## Bertopic

![](imagenes/bertopic.png){fig-align="center" width="317"}

Aquí encontrarás la [documentación](https://maartengr.github.io/BERTopic/index.html)

## Pasos de Bertopic

::: columns
::: {.column width="40%"}
![](imagenes/bertopic_pasos.svg){fig-align="center" width="483"}
:::

::: {.column width="60%"}
Utiliza todo lo que hemos aprendido hasta ahora
:::
:::

## Pasos

1.  Todos los textos son transformados en vectores con un modelo de lenguaje
2.  Se aplica un algoritmo de reducción de dimensionalidad (pca, umap, tsne u otro)
3.  Se aplica un algoritmo de clustering (DBSCAN, kmeans u otro)
4.  Vectorización de los textos
5.  Esquema de ponderación mediante c-TF-IDF
6.  Opcional: fine-tuning de los tópicos

## Paso 0: cargar datos

```{python, echo=TRUE}
import pandas as pd

# Cargar datos
data = pd.read_parquet("data/titulos.parquet")

# Hacemos una muestra de 500 documentos
data = data.sample(n = 500, random_state=42)
docs = list(data.text)
del data
print(docs[0:10])
```

## Paso 1: construir embeddings

```{python, echo = TRUE, output = FALSE }
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# Modelo de embeddings
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Generamos la instancia de BERTopic
topic_model = BERTopic(embedding_model=embedding_model)

# Generamos los embeddings
topic_model.fit_transform(docs)


```

```{python, echo=TRUE}
# Guardamos los embeddings en un un arreglo
embeddings =  topic_model._extract_embeddings(docs, method="document")
print(embeddings.shape)

```

## Paso 2: reducción de dimensionalidad

Aplicamos UMAP a la matriz de embeddings

```{python, echo=TRUE}
from umap import UMAP
umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')
umap_embeddings = umap_model.fit_transform(embeddings)
print(umap_embeddings.shape)


```

## Paso 3: clustering

Aplicamos HDBSCAN a la matriz reducida que viene de UMAP

El tópico -1 corresponde a *outliers*

```{python, echo=TRUE, output = FALSE}
from hdbscan import HDBSCAN
hdbscan_model = HDBSCAN(min_cluster_size=3, metric='euclidean', cluster_selection_method='eom', prediction_data=True)
hdbscan_model.fit(umap_embeddings);


```

```{python, echo=TRUE}
clusters = hdbscan_model.labels_
print(clusters)

```

## Paso 4: vectorización

Los textos son transformados en una matriz BoW

```{python, echo=TRUE}
from sklearn.feature_extraction.text import CountVectorizer
import nltk
from nltk.corpus import stopwords
import numpy as np

# Descargar stopwords en español
#nltk.download('stopwords') solo se descarga una vez
stopwords_es = stopwords.words('spanish')

# Creaamos la instancia del vectorizador
vectorizer_model = CountVectorizer(stop_words=stopwords_es, min_df = 3)

# Transformamos los documentos en una matriz de términos
X = vectorizer_model.fit_transform(docs)   


```



## Paso 4: vectorización (continuación)

```{python, echo=TRUE}
print("Las dimensiones de la matriz son: ", X.shape)

print("Algunas palabras del vocabulario:")

vocab = np.array(vectorizer_model.get_feature_names_out())

print(vocab[0:20])


```

## Paso 5: c-TF-IDF

Cada tópico es representado con un enfoque c-TF-IDF

El peso de la palabra $w$ en el tópico $c$ está dado por

$$
{W}_{w,c} = tf_{w,c} * log(1 + \frac{A}{f_{w}}) 
$$

-   $tf_{t,c}$: frecuencia de la palabra $x$ en la clase $c$
-   $A$: Promedio de palabras por clase
-   $f_{w}$: frecuencia de la palabra $w$ en todas las clases

## Paso 5: c-TF-IDF (continuación)

```{python, echo=TRUE}
# Excluimos los outliers
topic_ids = sorted([c for c in set(clusters) if c != -1])
topic_ids
```

```{python, echo=TRUE}

# Sumamos las frecuencias de cada palabra en cada tópico
X_topics = []
for c in topic_ids:
    idx = np.where(clusters == c)[0] # indices del cluster c
    X_topics.append(X[idx].sum(axis=0)) # sumar las frecuencias de cada palabra del cluster c

# COnvertir a una matriz de (n_topic, n_terms)
X_topics = [np.array(row).ravel() for row in X_topics]  # lista de (n_terms,)
X_topics = np.array(X_topics)

```



## Paso 5: c-TF-IDF (continuación)


```{python, echo=TRUE}
from bertopic.vectorizers import ClassTfidfTransformer
ctfidf = ClassTfidfTransformer().fit_transform(X_topics, len(docs))
print("Las dimensiones de la matriz son: ", ctfidf.shape)
```

## Todo junto

```{python, echo=TRUE, eval=FALSE}

from bertopic.representation import KeyBERTInspired

# Paso 1 - Extract embeddings
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Paso 2 - Reduce dimensionality
umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')

# Paso 3 - Cluster reduced embeddings
hdbscan_model = HDBSCAN(min_cluster_size=35, metric='euclidean', cluster_selection_method='eom', prediction_data=True)

# Paso 4 - Tokenize topics
vectorizer_model = CountVectorizer(stop_words=stopwords_es, min_df = 3)

# Paso 5 - Create topic representation
ctfidf_model = ClassTfidfTransformer()

# Step 6 - (Optional) Fine-tune topic representations with
# a `bertopic.representation` model
representation_model = KeyBERTInspired()

# All steps together
topic_model = BERTopic(
  embedding_model=embedding_model,          # Step 1 - Extract embeddings
  umap_model=umap_model,                    # Step 2 - Reduce dimensionality
  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings
  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics
  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words
  representation_model=representation_model # Step 6 - (Optional) Fine-tune topic representations
)

# Correr el modelo
topics, probs = topic_model.fit_transform(docs)
```


## Cargaremos el modelo ya entrenado

```{python, echo=TRUE}
import pandas as pd
import pickle
import numpy as np
import plotly.express as px
import datamapplot as dmp
import pickle
import io
import torch

```


```{python, echo=TRUE}

# Cargar títulos de noticias 
data = pd.read_parquet("data/titulos.parquet")
docs = list(data.text)
del data

# Cargar algunas cosas que están dentro del modelo
with open('data/bertopic_results.pkl', 'rb') as f:
    data = pickle.load(f)
    topics = data['topics']
    probs = data['probs']


# Cargar el modelo completo
#with open('data/bertopic_model.pkl', 'rb') as f:
#    topic_model = pickle.load(f)

# Clase para cargar el modelo en CPU
class CPU_Unpickler(pickle.Unpickler):
    def find_class(self, module, name):
        if module == 'torch.storage' and name == '_load_from_bytes':
            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')
        return super().find_class(module, name)

# Cargar el modelo de BERTopic en CPU
with open('data/bertopic_model.pkl', 'rb') as f:
    topic_model = CPU_Unpickler(f).load()


# Cargar títulos creados por gemini
with open("data/topic_titles.pkl", 'rb') as f:
    topic_titles = pickle.load(f)


# Cargar embeddings en d2
with open("data/embeddings_2d.pkl", "rb") as f:
    embeddings_2d = pickle.load(f)

print("Terminé de cargar todo")

```

## Trabajo previo a la visualización

```{python, echo=TRUE}

#  Obtener la información de los documentos
docs_info = topic_model.get_document_info(docs)

# Crear un diccionario con los tópicos a partir de la lista
topic_id_to_title =  {i:topic_titles[i + 1 ]  for i in range(-1, len(topic_titles) - 1)    }

docs_info["Topic_Title"] = docs_info["Topic"].map(topic_id_to_title)

# Obtén los clusters (topics)
topic_labels = np.array(topics)


# Filtra los outliers (topic -1)
mask = topic_labels != -1
embeddings_2d_filtered = embeddings_2d[mask]
topic_labels_filtered = topic_labels[mask]
custom_labels_filtered = docs_info["Topic_Title"][mask]
titles_filtered = np.array(docs)[mask]  # docs debe ser la lista de títulos

```

## Visualización interactiva

```{python, echo=TRUE}

# Acomodar los nombres de los clusters para el paquete de visualización
arr_str = custom_labels_filtered.astype(str)

# Armar la leyenda que aparece en el hover
combined_hover_text = [f"NOTICIA: {title}\nCLUSTER {text}" for title, text in zip(titles_filtered, arr_str)]

# Visualización
plot = dmp.create_interactive_plot(
    embeddings_2d_filtered,
    arr_str,
    hover_text=combined_hover_text
)

plot

```

## Más recursos

[Libro sobre text mining con R](https://www.tidytextmining.com/topicmodeling)

[Encontrando k óptimo](https://ladal.edu.au/topicmodels.html)

## En resumen

Hemos revisado varias estrategias para trabajar con texto

-   Procesamiento básico con stringr
-   Herramientas para POS con udpipe
-   Exploración y procesamiento de textos con quanteda
-   Transformación en vectores (tfidf)
-   Utilización de *embeddings* con Python (combinación con reticulate)
-   Algunas herramientas no supervisadas

. . .

**Cuentan con una serie de herramientas para desarrollar su trabajo final**

# Métodos computacionales para las ciencias sociales {.center background-color="aquamarine"}